{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b260e5-43fa-490c-8a87-a6373596f00d",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5978e29-ae67-4155-aa08-e4888e098dd0",
   "metadata": {},
   "source": [
    "Ans - Gradient Boosting Regression is a machine learning algorithm that is used for regression tasks. It is an extension of the Gradient Boosting framework, which was originally developed for classification problems. Gradient Boosting Regression builds an ensemble of weak regression models (typically decision trees) in a stage-wise manner to create a strong regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ba9df-06c3-4e6a-b631-93efeafed50c",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afeac3a7-6c48-4b34-af01-acb271c4898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.000000001411015\n",
      "R-squared: -3.5000000007055077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of y\n",
    "        prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - prediction\n",
    "            \n",
    "            # Fit a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the prediction\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            prediction += update\n",
    "            \n",
    "            # Store the weak model\n",
    "            self.models.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        # Make predictions using all weak models\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 3, 2, 4, 5])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45459f-9721-42e2-86b9-b7665c3bc584",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6efad7-efd3-4c30-aca2-f980dea84688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Mean Squared Error: 1.5492774342566584e-09\n",
      "R-squared: 0.9999999999950324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e42e35-a5fb-4edd-9a8f-f7fc93e6a12b",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb0d14-a7f7-4ae1-b716-663341571ec6",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-performing model that is used as a building block within the boosting framework. It is typically a model with low complexity, such as a decision tree with a small depth (often referred to as a decision stump), a linear model, or a shallow neural network.\n",
    "\n",
    "The concept of a weak learner in Gradient Boosting is different from the traditional notion of a weak classifier in other machine learning algorithms, such as AdaBoost. In Gradient Boosting, weak learners are not necessarily weak classifiers but rather models that are slightly better than random guessing on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c550c8-7910-46cd-8726-e0031bb5b37f",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc516532-cc29-4f93-bdd2-00cee3a31b46",
   "metadata": {},
   "source": [
    "Ans - **Ensemble Learning:** Gradient Boosting belongs to the family of ensemble learning methods, which aim to combine multiple weak models to create a strong predictive model. The basic principle is that by combining the predictions of several weak models, the ensemble can achieve better overall performance and generalization.\n",
    "\n",
    "**Iterative Improvement:** Gradient Boosting works in an iterative manner, with each iteration focusing on improving the weaknesses of the ensemble learned so far. It starts with an initial weak model and sequentially adds new models to the ensemble, with each new model designed to correct the mistakes made by the previous models.\n",
    "**\n",
    "**Gradient-Based Optimization:** The name \"Gradient Boosting\" stems from the fact that it leverages gradient-based optimization to improve the ensemble. At each iteration, the algorithm calculates the gradients (derivatives) of a specific loss function with respect to the current ensemble's predictions. These gradients provide information about the direction and magnitude of the improvement needed to minimize the loss.\n",
    "\n",
    "**Weak Learners as Building Blocks:** Gradient Boosting uses weak learners as building blocks. A weak learner is a model that performs slightly better than random guessing on the training data. By combining multiple weak learners, the algorithm progressively builds a strong ensemble model capable of capturing complex patterns and interactions in the data.\n",
    "\n",
    "**Sequential Training:** The weak learners are trained sequentially, where each new learner is trained to minimize the residuals (errors) of the ensemble learned so far. The residuals represent the information that has not been captured by the ensemble and are used as the target for training the next weak learner.\n",
    "\n",
    "**Aggregating Predictions:** The final prediction of the Gradient Boosting ensemble is obtained by aggregating the predictions of all the weak learners, each weighted according to its contribution to the ensemble. The weights are typically determined by the optimization process and depend on the performance of each learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59e762-2636-412c-b2af-6d223ee34e53",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda565f-fa71-44e0-a86a-c6a13df93a58",
   "metadata": {},
   "source": [
    "Ams - **1. Initialization:** The algorithm starts by initializing the ensemble with a simple model. This initial model can be a constant value, the mean of the target variable, or any other suitable value. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "**2. Calculation of Residuals:** For each iteration, the algorithm calculates the residuals, which represent the errors or discrepancies between the current ensemble's predictions and the true values of the target variable. Initially, the residuals are equal to the differences between the true values and the initial predictions.\n",
    "\n",
    "**3. Training of Weak Learners:** In each iteration, a weak learner is trained on the residuals of the previous iteration. The weak learner can be a decision tree, linear regression model, or any other model that can learn from the data and make predictions.\n",
    "\n",
    "**4. Gradient-Based Optimization:** The weak learner is trained to minimize the residuals by optimizing a specified loss function. This optimization is typically done using gradient-based methods such as gradient descent. The gradients of the loss function with respect to the predictions of the weak learner guide the optimization process.\n",
    "\n",
    "**5. Learning Rate and Model Contribution:** To control the contribution of each weak learner to the ensemble, a learning rate is introduced. The learning rate scales the predictions made by the weak learner, reducing their impact on the final prediction. A lower learning rate makes the algorithm more conservative and helps prevent overfitting.\n",
    "\n",
    "**6. Updating the Ensemble:** After training the weak learner, its predictions are multiplied by the learning rate and added to the current ensemble's predictions. This update step adjusts the ensemble's predictions to reduce the residuals. The weak learner's predictions are combined with the predictions of the previous models, and the ensemble gradually improves.\n",
    "\n",
    "**7. Iterative Process:** Steps 3 to 6 are repeated for a specified number of iterations, or until a stopping criterion is met. Each iteration focuses on reducing the residuals and improving the ensemble's predictions. The weak learners are trained on the residuals of the previous models, capturing the remaining information that has not been explained by the ensemble so far.\n",
    "\n",
    "**8. Final Ensemble:** The final ensemble is obtained by combining the predictions of all the weak learners. Each weak learner's prediction is scaled by the learning rate and added to the ensemble's predictions. The aggregated predictions form the final prediction of the Gradient Boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ada19-d27d-4dc2-a5ab-18e4b9d5a274",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763831e-f536-41af-9c4a-40704abf4d79",
   "metadata": {},
   "source": [
    "Here are the main steps to develop the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "**1. Loss Function:** Define a loss function that quantifies the difference between the predicted values and the true values of the target variable. The choice of loss function depends on the problem type (e.g., regression, classification). Common examples include mean squared error (MSE) for regression and cross-entropy loss for classification.\n",
    "\n",
    "**2. Initial Prediction:** Initialize the ensemble with an initial prediction, typically a constant value or the mean of the target variable. This initial prediction serves as the starting point for subsequent iterations.\n",
    "\n",
    "**3. Gradient Calculation:** Calculate the negative gradient (or the gradient with respect to the loss function) of the current ensemble's predictions with respect to the true values of the target variable. The negative gradient indicates the direction and magnitude of the changes needed to reduce the loss.\n",
    "\n",
    "**4. Training of Weak Learners:** Train a weak learner, such as a decision tree or a linear model, to fit the negative gradient obtained in the previous step. The weak learner aims to approximate the relationship between the input features and the negative gradient, effectively minimizing the loss function.\n",
    "\n",
    "**5. Learning Rate and Model Contribution:** Introduce a learning rate, typically denoted by a small value between 0 and 1. The learning rate scales the predictions of the weak learner, reducing their impact on the final prediction. A lower learning rate makes the algorithm more conservative and helps prevent overfitting.\n",
    "\n",
    "**6. Update the Ensemble:** Multiply the predictions of the weak learner by the learning rate and add them to the current ensemble's predictions. This update step adjusts the ensemble's predictions by a fraction of the weak learner's predictions, aiming to reduce the loss and improve the overall prediction.\n",
    "\n",
    "**7. Iterative Process:** Repeat steps 3 to 6 for a specified number of iterations or until a stopping criterion is met. In each iteration, the weak learner is trained on the negative gradient of the ensemble's predictions, and the ensemble is updated to incorporate the new predictions.\n",
    "\n",
    "**8. Final Ensemble:** The final ensemble is obtained by aggregating the predictions of all the weak learners, each scaled by the learning rate. The aggregated predictions form the final prediction of the Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc8509-1c5e-47a3-b831-a204c1a5bac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
