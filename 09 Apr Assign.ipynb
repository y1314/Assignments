{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf1aa71-6649-423a-82d9-c7001d2b89f2",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8fbe1-e658-4627-ab19-5e0b0e86be75",
   "metadata": {},
   "source": [
    "Ans - Bayes' theorem provides a way to update our belief or knowledge about the probability of a hypothesis H given new evidence E. It takes into account the prior probability of the hypothesis and how likely the evidence is to occur if the hypothesis is true. \n",
    "\n",
    "The general form of Bayes' theorem is as follows:\n",
    "\n",
    "P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(H|E) is the probability of the hypothesis H given the evidence E. This is often referred to as the posterior probability.\n",
    "- P(E|H) is the probability of observing the evidence E given that the hypothesis H is true. This is called the likelihood.\n",
    "- P(H) is the probability of the hypothesis H being true before considering any evidence. This is called the prior probability.\n",
    "- P(E) is the probability of observing the evidence E."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81245b1e-8d13-4425-a010-6660b60887bb",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36610344-da85-4ddb-802e-6e7b754123b7",
   "metadata": {},
   "source": [
    "Ans - P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(A|B) represents the conditional probability of event A given event B. This is known as the posterior probability.\n",
    "- P(B|A) denotes the conditional probability of event B given event A. This is called the likelihood.\n",
    "- P(A) represents the prior probability of event A.\n",
    "- P(B) represents the probability of event B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70871d-40a3-4fe2-b461-2531c7135f3e",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead6d9e-d80a-460f-9cd3-413633d32c3a",
   "metadata": {},
   "source": [
    "Ans - Bayes' theorem is used in various practical applications across different fields. Here are a few examples:\n",
    "\n",
    "**1. Medical Diagnosis:** Bayes' theorem is used in medical diagnosis to assess the probability of a disease given certain symptoms or test results. Doctors can incorporate prior knowledge about the prevalence of a disease, the accuracy of diagnostic tests, and the likelihood of observing specific symptoms to determine the probability of a patient having a particular condition.\n",
    "\n",
    "**2. Spam Filtering:** Email spam filters often employ Bayes' theorem to classify incoming emails as spam or non-spam. By analyzing the words and patterns in the email content, the filter calculates the probability that an email is spam based on previously observed spam and non-spam emails. This probability is updated using Bayes' theorem as new emails are received, enabling the filter to adapt and improve its accuracy.\n",
    "\n",
    "**3. Machine Learning:** Bayes' theorem serves as a foundation for various machine learning algorithms, such as Naive Bayes classifiers. These algorithms make predictions based on observed features and their conditional probabilities. Bayes' theorem helps estimate the probability of a particular class given the features, allowing the model to make informed predictions.\n",
    "\n",
    "**4. Fault Diagnosis:** In engineering and maintenance, Bayes' theorem is used for fault diagnosis in complex systems. By incorporating prior knowledge about system behavior, sensor data, and failure modes, engineers can update the probabilities of different fault hypotheses to identify the most likely cause of a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe5011-bf38-4c82-a59c-f2eb01df5d09",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf8b05-1041-4345-9cd4-d55c74d12afb",
   "metadata": {},
   "source": [
    "Ans -Bayes' theorem is closely related to conditional probability, as it provides a way to calculate the conditional probability of an event or hypothesis based on new evidence. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "\n",
    "Conditional probability is the probability of an event A occurring given that another event B has already occurred, and it is denoted as P(A|B). Bayes' theorem allows us to reverse the conditioning, calculating the probability of event B given that event A has occurred, which is denoted as P(B|A).\n",
    "\n",
    "Bayes' theorem can be derived from the definition of conditional probability using the concept of joint probability. The joint probability of two events A and B, denoted as P(A and B), is the probability that both A and B occur. I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6df064-f46d-45ff-b096-b1b8acd6d4ad",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8f9b8-1980-4000-b9f0-4d4c9265567b",
   "metadata": {},
   "source": [
    "Ans - To choose the appropriate Naive Bayes classifier, consider the nature of your features and whether the assumptions of each variant are valid for your data. Some guidelines to follow:\n",
    "\n",
    "- If your features are categorical or count-based (e.g., word frequencies), consider Multinomial Naive Bayes.\n",
    "- If your features are binary or Boolean (e.g., presence or absence of certain words), consider Bernoulli Naive Bayes.\n",
    "- If your features are continuous and approximately follow a Gaussian distribution, consider Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114bb32-68d2-49ec-bfab-7c96974f94cb",
   "metadata": {},
   "source": [
    "### Q6. Assignment:\n",
    "### You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c3e63-a1fc-4aa9-b277-bf3e27a7c13e",
   "metadata": {},
   "source": [
    "Let's calculate the probabilities step by step:\n",
    "\n",
    "1. Calculate the prior probabilities (assuming equal priors for each class):\n",
    "\n",
    "P(A) = P(B) = 0.5\n",
    "\n",
    "2. Calculate the likelihoods for each feature value given each class:\n",
    "\n",
    "P(X1 = 3 | A) = 4 / 16 = 0.25\n",
    "\n",
    "P(X1 = 3 | B) = 1 / 12 ≈ 0.083\n",
    "\n",
    "P(X2 = 4 | A) = 3 / 16 = 0.188\n",
    "\n",
    "P(X2 = 4 | B) = 3 / 12 = 0.25\n",
    "\n",
    "3. Calculate the probability of the new instance for each class using the naive assumption of independence:\n",
    "\n",
    "P(X1 = 3, X2 = 4 | A) = P(X1 = 3 | A) * P(X2 = 4 | A) ≈ 0.25 * 0.188 ≈ 0.047\n",
    "\n",
    "P(X1 = 3, X2 = 4 | B) = P(X1 = 3 | B) * P(X2 = 4 | B) ≈ 0.083 * 0.25 ≈ 0.021\n",
    "\n",
    "4. Apply Bayes' theorem to calculate the posterior probabilities:\n",
    "\n",
    "P(A | X1 = 3, X2 = 4) = (P(X1 = 3, X2 = 4 | A) * P(A)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "P(B | X1 = 3, X2 = 4) = (P(X1 = 3, X2 = 4 | B) * P(B)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "Since the denominators P(X1 = 3, X2 = 4) are the same for both classes, we can compare the numerators directly:\n",
    "\n",
    "Numerator for class A: P(X1 = 3, X2 = 4 | A) * P(A) ≈ 0.047 * 0.5 ≈ 0.0235\n",
    "\n",
    "Numerator for class B: P(X1 = 3, X2 = 4 | B) * P(B) ≈ 0.021 * 0.5 = 0.0105\n",
    "\n",
    "Comparing the numerators, we see that the numerator for class A is larger than that for class B. Therefore, according to Naive Bayes, the new instance with features X1 = 3 and X2 = 4 would be predicted to belong to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
